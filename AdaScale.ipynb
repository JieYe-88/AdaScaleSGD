{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "from typing import TYPE_CHECKING, Any, Dict, List, Optional, Type\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.distributed as dist\n",
    "from torch.optim import SGD, Optimizer\n",
    "\n",
    "if TYPE_CHECKING:  # pragma: no cover\n",
    "    from torch.optim.optimizer import _params_t\n",
    "else:\n",
    "    _params_t = Any\n",
    "\n",
    "\n",
    "class AdaScale(Optimizer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        world_size: Optional[int] = None,\n",
    "        scale: Optional[float] = None,\n",
    "        smoothing: float = None,\n",
    "        num_gradients_to_accumulate: int = 1,\n",
    "        debias_ewma: bool = True,\n",
    "    ):\n",
    "        self._optimizer = optimizer\n",
    "        self._local_grad_sqr: Optional[torch.Tensor] = None\n",
    "        self._world_size: int = (\n",
    "            world_size if world_size is not None else 1 if not dist.is_available() else dist.get_world_size() if dist.is_initialized() else 1\n",
    "        )\n",
    "        self._num_backward_calls = 0\n",
    "        self._last_final_backward_call = 0\n",
    "        self._num_grads_to_accum = num_gradients_to_accumulate\n",
    "        self._debias_ewma = debias_ewma\n",
    "\n",
    "        # Proxy the param_groups so that `torch.optim.lr_scheduler` can work.\n",
    "        self.param_groups = self._optimizer.param_groups\n",
    "\n",
    "        self.set_num_gradients_to_accumulate(num_gradients_to_accumulate, update_smoothing=True)\n",
    "\n",
    "        # The previous function call sets smoothing to its default value.\n",
    "        # Override that here if smoothing was passed as an argument.\n",
    "        if smoothing is not None:\n",
    "            self._smoothing = smoothing\n",
    "\n",
    "        if self._world_size * self._num_grads_to_accum <= 1:\n",
    "            # gain will be NaN since we will be dividing by zero in paper's B.3 where (S-1) == 0.\n",
    "            raise RuntimeError(\"AdaScale does not support a single worker without grad accumulation.\")\n",
    "\n",
    "        # Per-param-group sqr & var states (sigma^2 & mu^2 in the paper).\n",
    "        self._optimizer.state.setdefault(\n",
    "            \"adascale\",\n",
    "            {\n",
    "                \"grad_sqr_avg\": np.ones(len(optimizer.param_groups)),\n",
    "                \"grad_var_avg\": np.zeros(len(optimizer.param_groups)),\n",
    "            },\n",
    "        )\n",
    "\n",
    "        self._scale = 1.0  # Assign to inform mypy about the typing of this variable.\n",
    "        self.set_scale(self._world_size * self._num_grads_to_accum if scale is None else scale)\n",
    "\n",
    "        self._hook_handles: List[Any] = []\n",
    "        self._hook()\n",
    "\n",
    "    def _hook(self) -> None:\n",
    "        assert self._hook_handles == [], \"Must run unhook first\"\n",
    "        for idx, param_group in enumerate(self._optimizer.param_groups):\n",
    "            for param in param_group[\"params\"]:\n",
    "                h = param.register_hook(functools.partial(self._backward_hook, idx))\n",
    "                self._hook_handles.append(h)\n",
    "\n",
    "    def __del__(self) -> None:\n",
    "        self.unhook()\n",
    "\n",
    "    def unhook(self) -> None:\n",
    "        for h in self._hook_handles:\n",
    "            h.remove()\n",
    "        self._hook_handles = []\n",
    "\n",
    "    @property\n",
    "    def _state(self) -> Dict[str, np.ndarray]:\n",
    "        return self._optimizer.state[\"adascale\"]\n",
    "\n",
    "    @property\n",
    "    def scale(self) -> float:\n",
    "        return self._scale\n",
    "\n",
    "    @property\n",
    "    def smoothing(self) -> float:\n",
    "        return self._smoothing\n",
    "\n",
    "    def set_scale(self, scale: float, update_estimate: bool = True) -> None:\n",
    "        assert self._local_grad_sqr is None, \"Don't change scale in backward phase\"\n",
    "        assert scale >= 1, \"Scale must be at least 1\"\n",
    "        if update_estimate and hasattr(self, \"_scale\"):\n",
    "            assert self._scale >= 1, \"bug: old scale isn't valid\"\n",
    "            # Rescale grad_var_avg to account for the change in scale\n",
    "            if self._debias_ewma and \"grad_var_avg_biased\" in self._state:\n",
    "                self._state[\"grad_var_avg_biased\"] *= self._scale / scale\n",
    "            elif \"grad_var_avg_total\" in self._state:  # _debias_ewma==False\n",
    "                self._state[\"grad_var_avg_total\"] *= self._scale / scale\n",
    "            self._state[\"grad_var_avg\"] *= self._scale / scale\n",
    "        self._scale = scale\n",
    "\n",
    "    def _grad_sqr_avg(self, pg_idx: Optional[int] = None) -> float:\n",
    "        if pg_idx is not None:\n",
    "            return self._state[\"grad_sqr_avg\"][pg_idx]\n",
    "        else:\n",
    "            return float(np.sum(self._state[\"grad_sqr_avg\"]))\n",
    "\n",
    "    def _grad_var_avg(self, pg_idx: Optional[int] = None) -> float:\n",
    "        if pg_idx is not None:\n",
    "            return self._state[\"grad_var_avg\"][pg_idx]\n",
    "        else:\n",
    "            return float(np.sum(self._state[\"grad_var_avg\"]))\n",
    "\n",
    "    def gain(self, pg_idx: Optional[int] = None) -> float:\n",
    "        var = self._grad_var_avg(pg_idx)\n",
    "        sqr = self._grad_sqr_avg(pg_idx)\n",
    "        gain = (var + sqr) / (var / self.scale + sqr)\n",
    "        return gain\n",
    "\n",
    "    def _update_avg(self, name: str, value: np.ndarray, factor: float) -> None:\n",
    "        if self._debias_ewma:\n",
    "            # This function computes and stores the moving average of a vector\n",
    "            # using a smoothing factor.\n",
    "            biased = self._state.get(name + \"_biased\", np.zeros(value.shape[0]))\n",
    "            unbias = self._state.get(name + \"_unbias\", np.zeros(value.shape[0]))\n",
    "            biased = factor * biased + (1.0 - factor) * value\n",
    "            unbias = factor * unbias + (1.0 - factor)\n",
    "            self._state[name + \"_biased\"] = biased\n",
    "            self._state[name + \"_unbias\"] = unbias\n",
    "            self._state[name] = biased / unbias\n",
    "        else:\n",
    "            count = self._state.get(name + \"_count\", np.zeros(1))\n",
    "            count[0] += 1\n",
    "            self._state[name + \"_count\"] = count\n",
    "            if count < 1 / (1 - self._smoothing):\n",
    "                total = self._state.get(name + \"_total\", None)\n",
    "                if total is None:\n",
    "                    total = value\n",
    "                else:\n",
    "                    total += value\n",
    "                self._state[name + \"_total\"] = total\n",
    "                self._state[name] = total / count\n",
    "            else:\n",
    "                self._state[name] = factor * self._state[name] + (1.0 - factor) * value\n",
    "\n",
    "    def _backward_hook(self, pg_idx: int, grad: torch.Tensor) -> None:\n",
    "        if self._local_grad_sqr is None:\n",
    "            self._local_grad_sqr = torch.zeros(\n",
    "                len(self._optimizer.param_groups),\n",
    "                device=grad.device,\n",
    "                requires_grad=False,\n",
    "            )\n",
    "        self._local_grad_sqr[pg_idx] += grad.pow(2).sum()\n",
    "        self._final_callback_queued = False\n",
    "        Variable._execution_engine.queue_callback(self._queue_callback)\n",
    "\n",
    "    def _queue_callback(self) -> None:\n",
    "        if self._final_callback_queued:\n",
    "            return\n",
    "        self._final_callback_queued = True\n",
    "        Variable._execution_engine.queue_callback(self._final_callback)\n",
    "\n",
    "    def _final_callback(self) -> None:\n",
    "        self._final_callback_queued = False\n",
    "        assert isinstance(self._local_grad_sqr, torch.Tensor)\n",
    "        self._num_backward_calls += 1\n",
    "        assert (\n",
    "            self._num_backward_calls - self._last_final_backward_call\n",
    "        ) <= self._num_grads_to_accum, (\n",
    "            f\"bug: {self._num_backward_calls} - {self._last_final_backward_call} should <= {self._num_grads_to_accum}\"\n",
    "        )\n",
    "        if (self._num_backward_calls - self._last_final_backward_call) % self._num_grads_to_accum != 0:\n",
    "            assert self._local_grad_sqr is not None, \"We should still be in backward phase\"\n",
    "            return\n",
    "\n",
    "        work = None\n",
    "        if self._world_size > 1:\n",
    "            work = dist.all_reduce(self._local_grad_sqr, async_op=True)  # SUM\n",
    "\n",
    "        total_grad_sqr = np.array(\n",
    "            [sum(param.grad.pow(2).sum().item() for param in group[\"params\"]) for group in self._optimizer.param_groups]\n",
    "        )\n",
    "\n",
    "        if self._num_grads_to_accum > 1:\n",
    "            # np array doesn't support /=.\n",
    "            total_grad_sqr = total_grad_sqr / (self._num_grads_to_accum ** 2)\n",
    "\n",
    "        # Wait for all_reduce to be done and move it to cpu & np.\n",
    "        if work:\n",
    "            work.wait()\n",
    "        local_grad_sqr = self._local_grad_sqr.cpu().numpy()\n",
    "\n",
    "        S = self._scale\n",
    "        cN = self._world_size * self._num_grads_to_accum\n",
    "        grad_var = local_grad_sqr * (S / cN) / (cN - 1) - total_grad_sqr * S / (cN - 1)\n",
    "        grad_sqr = total_grad_sqr - grad_var / S\n",
    "        grad_var = np.maximum(grad_var, 1e-6)\n",
    "        grad_sqr = np.maximum(grad_sqr, 0.0)\n",
    "        self._update_avg(\"grad_sqr_avg\", grad_sqr, self.smoothing)\n",
    "        self._update_avg(\"grad_var_avg\", grad_var, self.smoothing)\n",
    "        self._last_final_backward_call = self._num_backward_calls\n",
    "        # Indicating backward is done.\n",
    "        self._local_grad_sqr = None\n",
    "\n",
    "    def step(self, *args: Any, **kwargs: Any) -> Optional[float]:\n",
    "        assert self._local_grad_sqr is None, \"Don't step without finishing backward phase\"\n",
    "        # Set original LR and set new LR.\n",
    "        original_lr = []\n",
    "        for idx, param_group in enumerate(self._optimizer.param_groups):\n",
    "            original_lr.append(param_group[\"lr\"])\n",
    "            param_group[\"lr\"] = self.gain(pg_idx=idx) * param_group[\"lr\"]\n",
    "\n",
    "        # Step it.\n",
    "        res = self._optimizer.step(*args, **kwargs)\n",
    "\n",
    "        # Restore the original LR.\n",
    "        for lr, param_group in zip(original_lr, self._optimizer.param_groups):\n",
    "            param_group[\"lr\"] = lr\n",
    "\n",
    "        return res\n",
    "\n",
    "    def add_param_group(self, pg: Dict) -> None:\n",
    "        assert self._local_grad_sqr is None, \"Can't add parameter group during backward\"\n",
    "        self._optimizer.add_param_group(pg)\n",
    "        # Update the hooks.\n",
    "        self.unhook()\n",
    "        self._hook()\n",
    "        # Extend the states.\n",
    "        for name in self._state.keys():\n",
    "            assert name.startswith(\"grad_sqr_avg\") or name.startswith(\"grad_var_avg\"), name\n",
    "            if name.endswith(\"_count\"):\n",
    "                # This is the \"_count\" variable, should be a 1D int.\n",
    "                assert self._state[name].shape == (1,), self._state[name].shape\n",
    "                continue\n",
    "            # must be a np array, extend it with the right value and check the shape.\n",
    "            val = 1 if name == \"grad_sqr_avg\" else 0\n",
    "            self._state[name] = np.append(self._state[name], val)  # type: ignore\n",
    "            assert self._state[name].shape == (len(self._optimizer.param_groups),)\n",
    "\n",
    "    def zero_grad(self) -> None:\n",
    "        assert self._local_grad_sqr is None, \"Don't zero_grad in backward\"\n",
    "        return self._optimizer.zero_grad()\n",
    "\n",
    "    def state_dict(self) -> Dict:\n",
    "        assert self._local_grad_sqr is None, \"Don't checkpoint in backward\"\n",
    "        return self._optimizer.state_dict()\n",
    "\n",
    "    def load_state_dict(self, data: Dict) -> None:\n",
    "        assert self._local_grad_sqr is None, \"Don't load checkpoint in backward\"\n",
    "        return self._optimizer.load_state_dict(data)\n",
    "\n",
    "    def set_num_gradients_to_accumulate(\n",
    "        self,\n",
    "        num_gradients_to_accumulate: int,\n",
    "        update_smoothing: bool = True,\n",
    "    ) -> None:\n",
    "        assert self._local_grad_sqr is None, \"Don't change num_grad_to_accum in backward\"\n",
    "        assert num_gradients_to_accumulate >= 1, f\"Invalid value {num_gradients_to_accumulate}\"\n",
    "        self._num_grads_to_accum = num_gradients_to_accumulate\n",
    "        if update_smoothing:\n",
    "            self._smoothing = max(1 - self._world_size * self._num_grads_to_accum / 1000, 0)\n",
    "\n",
    "    def __getattr__(self, name: str) -> Any:\n",
    "        \"\"\"Forward missing attributes to wrapped optimizer.\"\"\"\n",
    "        try:\n",
    "            return super().__getattr__(name)  # defer to Optimizer logic\n",
    "        except AttributeError:\n",
    "            return getattr(self._optimizer, name)  # fallback to wrapped optim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "mlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}